{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"NEON Workshop","text":"<p>This page uses a custom template and will display the workshop landing page.</p>"},{"location":"neon-aop-gee/","title":"NEON Airborne Remote Sensing in GEE Tutorials","text":"<p>Open Science Initiative</p> <p>Welcome to the NEON Airborne Remote Sensing in Google Earth Engine Resources workshop! This community-driven resource provides comprehensive tutorials for working with AOP raster data products in Google Earth Engine.</p>"},{"location":"neon-aop-gee/#about-this-repository","title":"About This Repository","text":"<p>This workshop offers hands-on tutorials to help researchers, students, and practitioners work with NEON Airborne Observation Platform (AOP) data products in Google Earth Engine. You can find the slide deck here. Our tutorials cover data generated from three primary sensors:</p> <ul> <li> <p> Hyperspectral Imaging</p> <p>High-resolution spectral data for vegetation analysis and species classification</p> <p>426 spectral bands | 1m spatial resolution</p> </li> <li> <p> LiDAR Sensors</p> <p>3D structural data for canopy height and biomass estimation</p> <p>Point clouds | Derived products</p> </li> <li> <p> RGB Camera</p> <p>High-resolution imagery for visual context and ground truthing</p> <p>10cm spatial resolution | True color</p> </li> </ul>"},{"location":"neon-aop-gee/#repository-status","title":"Repository Status","text":"<p>Active Development</p> <p>This repository is publicly available in the interest of open science but remains under active development.</p> <ul> <li> Check the changelog for recent updates</li> <li> Community contributions are welcome</li> <li> Report issues via GitHub Issues</li> </ul>"},{"location":"neon-aop-gee/#contact-support","title":"Contact &amp; Support","text":""},{"location":"neon-aop-gee/#neon-airborne-observation-platform","title":"NEON Airborne Observation Platform","text":"<p>Primary Contact</p> <p>Organization: National Ecological Observatory Network Airborne Observation Platform (NEON AOP)\u00b9</p> <p> Website: neonscience.org</p> <p>Contact Options:</p> <p> General Contact Form</p> <p> AOP GEE List</p>"},{"location":"neon-aop-gee/#spatial-bytes","title":"Spatial Bytes","text":"<p>Development Partner</p> <p>Website: https://contact.spatialbytes.work/</p> <p>Funding Acknowledgment</p> <p>\u00b9 NEON is a project fully funded by the National Science Foundation and operated by Battelle.</p>"},{"location":"background/aop_background/","title":"NEON Airborne Observation Platform","text":""},{"location":"background/aop_background/#neon-airborne-observation-platform-aop","title":"NEON Airborne Observation Platform (AOP)","text":"NEON Airborne Remote Sensing"},{"location":"background/aop_background/#aop-payload-sensors","title":"AOP Payload Sensors","text":"<p>The AOP consists of three complete and comparable instrument payloads. Typically, two of the payloads are dedicated to collections of the NEON field sites while the third is dedicated to NEON's Research Support services which support externally driven research. The primary sensors on each payload include:</p> <ol> <li>A discrete and full-waveform lidar to provide three-dimensional structural information of the landscape</li> <li>An imaging spectrometer to allow discrimination of land cover types and chemical content of vegetation</li> <li>A high-resolution digital camera to provide spatially accurate and detailed contextual information</li> <li>A GPS antenna and receiver and Inertial Measurement Unit (IMU) to provide high-accuracy positioning and orientation of the aircraft</li> </ol>"},{"location":"background/aop_background/#aop-data-products","title":"AOP Data Products","text":"<p>The AOP produces approximately 30 data products. The products are separated into categories of Level 1, Level 2, and Level 3 (L1, L2, L3). L1 represents the least processed data products. Additional processing steps are required to transition the L1 data to the derived L2 and L3 data. Broadly, the L1 and L2 products are provided by individual aircraft flight line, while L3 products are provided in 1 km by 1 km tiles. Generally, the data volume for L1 products is the highest and decreases for L2 and L3 products.</p> <p>Data Access</p> <p>Details of the different products within each Level can be found in the individual webpages for each sensor. All AOP data products can be found on the NEON Data Portal, and a subset of the L3 data products are available on Google Earth Engine.</p>"},{"location":"background/aop_background/#imaging-spectrometer-data-products","title":"Imaging Spectrometer Data Products","text":"<p>Level 1 (L1) products include at-sensor radiance and surface reflectance which are distributed by flightline. The image data is georeferenced to the ITRF00 datum and projected into the appropriate UTM zone, and provided at 1 m spatial resolution. Both the radiance and reflectance image data are stored in an HDF5 file format that includes extensive metadata and data quality information. The HDF5 format was selected because of the flexibility it allows in storing associated metadata.</p> <p>Level 2 (L2) products are derived from the L1 surface reflectance and are produced at the same spatial resolution (1 m), datum and map projection as the Level 1 products. The L2 products include a suite of spectral indices designed to strategically combine bands to highlight vegetation characteristics such as photosynthetic activity or water content. For example, NDVI (Normalized Difference Vegetation Index) is a well-known and commonly used vegetation index which combines information from the NIR and Red regions to estimate vegetative greenness and can be used as a proxy for plant health. The L2 products also include fPAR (fraction of photosynthetically active radiation) and LAI (leaf area index), products further derived from vegetation indices. Additionally, a surface Albedo product that estimates the integrated reflectance of all the NIS bands into a single value is also provided. All L2 products are distributed by flightline in a GeoTIFF (gtiff) format. Currently, all vegetation indices, water indices, fPAR, and LAI are delivered with associated simulated error images.</p> <p>Level 3 (L3) products include mosaics of all L1 and L2 products, excluding at-sensor radiance, and are distributed as 1 km x 1 km tiles instead of flightlines. Tiles are created by making a full mosaic of all the data and sub-setting the 1 km x 1 km tiles. The tiles are designed so their boundaries are set to even 1000 m UTM coordinate intervals. During the mosaic generation, the algorithm preferentially selects pixels that were collected under the best weather conditions in regions with multiple potential pixels due to flightline overlap. If weather conditions were equivalent, pixels acquired nearest to nadir of the image acquisition are selected. Generally, this will correspond to pixels that are nearest to the center of the flightline. The tiles are created at the same spatial resolution (1 m) as the L1 and L2 products are in delivered in gtiff format, with the exception of the surface reflectance, which is delivered in HDF5 format.</p>"},{"location":"background/aop_background/#brdf-and-topographic-corrections","title":"BRDF and Topographic Corrections","text":"<p>BRDF Corrections Available</p> <p>Starting in 2024, NEON began producing BRDF (Bidirectional Reflectance Distribution Function) and topographic corrected reflectance data, which include \"bidirectional\" in the name, and end with revision .002 in the Data Product IDs.</p> <p>As of 2025, these bidirectional reflectance are currently only available for data collected between 2022-2024. NEON is beginning to back-process earlier years (pre-2022) to apply the BRDF and topographic corrections. Please look at the data availability charts for each product on the data portal to determine whether the bidirectional data are available. Eventually, only bidirectional data products will be delivered, with the exception of the Level 1 Spectrometer orthorectified surface directional reflectance (DP1.30006.001), which will continue to be delivered, so that researchers who wish to carry out their own BRDF, topographic, or other corrections may do so.</p> <p>The table below shows a full list of NEON's spectrometer-derived data products, including the corresponding bidirectional reflectance data products, if applicable.</p> <p>Table 1: NEON AOP Imaging Spectrometer Datasets</p> Product Name Level Data Product ID (DPID) BRDF-Corrected DPID Spectrometer orthorectified at-sensor radiance L1 DP1.30008.001 - Spectrometer orthorectified surface (bi)directional reflectance L1 DP1.30006.001 DP1.30006.002 Albedo - spectrometer - flightline L2 DP2.30011.001 DP2.30011.002 LAI - spectrometer - flightline L2 DP2.30012.001 DP2.30012.002 fPAR - spectrometer - flightline L2 DP2.30014.001 DP2.30014.002 Canopy water indices - flightline L2 DP2.30019.001 DP2.30019.002 Vegetation indices - spectrometer - flightline L2 DP2.30026.001 DP2.30026.002 Albedo - spectrometer - mosaic L3 DP3.30011.001 DP3.30011.002 LAI - Spectrometer - mosaic L3 DP3.30012.001 DP3.30012.002 fPAR - spectrometer - mosaic L3 DP3.30014.001 DP3.30014.002 Canopy water indices - mosaic L3 DP3.30019.001 DP3.30019.002 Vegetation indices - spectrometer - mosaic L3 DP3.30026.001 DP3.30026.002 <p>In addition to the spectrometer-derived data products, NEON generates 5 lidar-derived products and 2 RGB camera data products, summarized below. These data products provide valuable structural and visual information that compliment the spectrometer data.</p>"},{"location":"background/aop_background/#lidar-data-products","title":"LiDAR Data Products","text":"<p>Table 2: NEON AOP Lidar Datasets</p> Product Name Level Data Product ID (DPID) ATBD Document # LiDAR Slant Range Waveform L1 DP1.30001.001 NEON.DOC.001293 Discrete Return LiDAR Point Cloud L1 DP1.30003.001 NEON.DOC.001292, NEON.DOC.001288 Ecosystem Structure L3 DP3.30015.001 NEON.DOC.002387 Elevation \u2013 LiDAR L3 DP3.30024.001 NEON.DOC.002390 Slope and Aspect \u2013 LiDAR L3 DP3.30025.001 NEON.DOC.003791"},{"location":"background/aop_background/#rgb-camera-products","title":"RGB Camera Products","text":"<p>Table 3: NEON AOP Camera Datasets</p> Product Name Level Data Product ID (DPID) ATBD Document # High-resolution orthorectified camera imagery L1 DP1.30010.001 NEON.DOC.001211vB High-resolution orthorectified camera imagery mosaic L3 DP3.30010.001 NEON.DOC.005052vB"},{"location":"background/aop_background/#summary","title":"Summary","text":"<p>The NEON Airborne Observation Platform represents a cutting-edge approach to continental-scale ecological monitoring, providing researchers with unprecedented access to high-resolution, standardized remote sensing data across diverse ecosystems. The integration of lidar, imaging spectrometry, and high-resolution camera systems creates a comprehensive dataset that enables detailed analysis of ecosystem structure, function, and change over time.</p> <p>The multi-level data processing approach (L1, L2, L3) ensures that researchers can access data at the appropriate level of processing for their specific research needs, from raw sensor measurements to analysis-ready mosaicked products. The recent addition of BRDF and topographic corrections further enhances the scientific value of these datasets by improving comparability across different viewing and illumination conditions.</p> <p>Getting Started</p> <p>To begin working with NEON AOP data, visit the NEON Data Portal to explore available datasets, or check out the Google Earth Engine catalog for cloud-based analysis of L3 products.</p> <p>Additional Resources</p> <p>For detailed technical information about data processing algorithms and product specifications, refer to the ATBD (Algorithm Theoretical Basis Document) numbers listed in the tables above, available through the NEON Data Portal.</p>"},{"location":"background/neon_background/","title":"What is NEON?","text":"<p>NEON is a continental-scale observation facility designed to collect long-term open-access ecological data to better understand the complexities of Earth's ecosystems and how they are changing. NEON uses cutting-edge sensor networks, instrumentation, observational sampling, natural history archive facilities and remote sensing methods and technologies to collect data on plants, animals, soil, nutrients, freshwater and the atmosphere.</p> <p>NEON operates 81 field sites strategically located across 20 ecoclimatic Domains across the United States, including 47 terrestrial sites and 34 freshwater aquatic sites. When logistically possible, aquatic and terrestrial field sites are colocated (i.e. in close proximity) to support understanding of linkages across terrestrial and aquatic ecosystems and their interactions with the atmosphere. For example, Domain 08, the Ozarks Complex, has three co-located sets of terrestrial and aquatic field sites. These sites are situated along the same watershed system, creating a unique opportunity to study hydrology, nutrient transport, and biogeochemical cycling through the watershed.</p> <p> </p> NEON Field Sites Map - Green: Terrestrial Sites, Blue: Aquatic Sites <p>NEON delivers data products from three main sub-systems called the Airborne Observation Platform (AOP), Terrestrial and Aquatic Observational Systems (TOS/AOS), and the Instrumented Systems (TIS/AIS). The section below provides a brief summary of these sub-systems.</p>"},{"location":"background/neon_background/#neon-airborne-observation-platform-aop","title":"NEON Airborne Observation Platform (AOP)","text":"NEON Airborne Remote Sensing <p>Airborne remote sensing surveys are conducted over NEON field sites during peak greenness and provide quantitative information on land cover and changes to ecological structure and chemistry, including the presence and effects of invasive species. The surveys are supported by the NEON Airborne Observation Platform (AOP), a suite of earth observation instruments installed into a Twin Otter aircraft designed to collect high-resolution remote sensing data at low altitude. AOP was designed to collect regional-scale landscape information at the NEON field sites. The AOP maps areas where NEON's observational and instrumented sampling is occurring and allows relationships to be drawn between NEON's detailed in-situ observations to the broader environmental and ecological conditions.</p> <p>Learn More</p> <p>Please see the next section Airborne Observation Platform for more details on the AOP including a summary of the data products provided.</p>"},{"location":"background/neon_background/#neon-field-data","title":"NEON Field Data","text":"<p>In addition to the AOP remote sensing data, NEON also provides Observational Sampling (OS) data and Instrumented Sampling (IS) data at terrestrial and aquatic sites. The field and instrumented sampling are briefly described below, but we encourage exploring the NEON website further for a more detailed understanding of the sensors and data products provided by the OS and IS groups.</p>"},{"location":"background/neon_background/#observational-sampling","title":"Observational Sampling","text":"NEON Observational Samples <p>NEON field scientists collect a broad variety of observations and samples at terrestrial and aquatic field sites at regular intervals throughout the year. The data and samples collected by NEON's Aquatic Observation System (AOS) and Terrestrial Observation System (TOS) are designed to provide standardized, continentally distributed observations of organisms, biogeochemistry, and physical properties.</p>"},{"location":"background/neon_background/#instrumented-sampling","title":"Instrumented Sampling","text":"NEON Instrumented Sampling <p>NEON deploys automated instruments to collect meteorological, soil, phenological, surface water, and groundwater data at NEON field sites.</p> <p>Colocated Sites</p> <p>Where logistically possible, NEON colocated aquatic sites with terrestrial sites (21 in total) to support an understanding of linkages across atmospheric, terrestrial, and aquatic ecosystems. The suite of OS, IS, and AOP data provide an unparalleled opportunity to study ecosystem-level change over time in the United States.</p>"},{"location":"gee/gee_catalog/","title":"Earth Engine Data Catalog 90+ Petabytes of Geospatial Data","text":"<p>Catalog Overview</p> <p>1000+ datasets \u2022 90+ petabytes of data \u2022 Continuously updated \u2022 Free access to most datasets</p> <p>The Earth Engine data catalog represents one of the world's largest collections of publicly available geospatial datasets. Originally designed around primary datasets curated by the Earth Engine team, the catalog has grown into a comprehensive repository that democratizes access to satellite imagery, climate data, and geophysical datasets from around the globe.</p> <p> </p> Exploring the Earth Engine Data Catalog interface"},{"location":"gee/gee_catalog/#dataset-categories","title":"Dataset Categories","text":"<p>The catalog spans multiple domains of Earth observation and environmental monitoring:</p> Satellite ImageryClimate &amp; WeatherTerrain &amp; TopographyEnvironmental <ul> <li>Landsat Collection - 50+ years of Earth observation</li> <li>Sentinel Series - High-resolution European Space Agency data</li> <li>MODIS - Daily global coverage for environmental monitoring</li> <li>Planet - High-frequency, high-resolution commercial imagery</li> </ul> <ul> <li>ERA5 Reanalysis - Comprehensive atmospheric data</li> <li>CHIRPS - Precipitation datasets for climate monitoring</li> <li>Temperature Records - Historical and real-time temperature data</li> <li>Weather Station Data - Ground-based meteorological observations</li> </ul> <ul> <li>SRTM - Global digital elevation models</li> <li>ASTER GDEM - High-resolution terrain data</li> <li>Global Forest Change - Annual forest loss and gain</li> <li>Land Cover Classifications - Multiple global and regional products</li> </ul> <ul> <li>Air Quality - Pollution and atmospheric composition</li> <li>Ocean Data - Sea surface temperature, salinity, currents</li> <li>Biodiversity - Species distribution and habitat data</li> <li>Urban Development - Built environment and infrastructure</li> </ul>"},{"location":"gee/gee_catalog/#finding-your-data","title":"Finding Your Data","text":""},{"location":"gee/gee_catalog/#quick-search-methods","title":"Quick Search Methods","text":"<p>Search Strategies</p> <p>\ud83d\udd0d By Keywords: Use terms like \"landsat\", \"temperature\", \"precipitation\"</p> <p>\ud83d\uddd3\ufe0f By Date Range: Filter datasets by temporal coverage</p> <p>\ud83c\udf0d By Geographic Area: Search for region-specific datasets</p> <p>\ud83d\udcca By Data Type: Filter by imagery, tables, or image collections</p> <p> </p> Searching for datasets directly in the Earth Engine Code Editor"},{"location":"gee/gee_catalog/#access-points","title":"Access Points","text":"<p>Where to Find Data</p> <p>\ud83c\udf10 Main Catalog Browser - Web-based dataset explorer</p> <p>\ud83d\udccb Complete Dataset List - All available datasets in one view</p> <p>\ud83d\udcbb Code Editor Search - Built-in search within the development environment</p> <p>\ud83d\udcd6 Dataset Tags - Browse by thematic categories</p>"},{"location":"gee/gee_catalog/#specialized-collections","title":"Specialized Collections","text":"<p>Domain-Specific Highlights</p> <p>\ud83c\udf31 Agriculture: Crop type maps, NDVI time series, soil moisture</p> <p>\ud83c\udfd9\ufe0f Urban Planning: Nighttime lights, population density, built-up areas</p> <p>\ud83c\udf0a Water Resources: Surface water occurrence, flood mapping, water quality</p> <p>\ud83d\udd25 Disaster Response: Fire detection, damage assessment, emergency mapping</p>"},{"location":"gee/gee_catalog/#data-licensing-and-usage","title":"Data Licensing and Usage","text":""},{"location":"gee/gee_catalog/#understanding-access-rights","title":"Understanding Access Rights","text":"<p>Important Licensing Information</p> <ul> <li>Most datasets are free for research and non-commercial use</li> <li>Commercial datasets require specific licensing (clearly marked)</li> <li>Citation requirements vary by dataset - check individual dataset pages</li> <li>Usage limits may apply to some high-resolution commercial data</li> </ul>"},{"location":"gee/gee_catalog/#best-practices","title":"Best Practices","text":"Before Using DataFor Reproducible Research <ol> <li>Read the dataset description thoroughly</li> <li>Check temporal and spatial coverage for your area of interest</li> <li>Review licensing terms and citation requirements</li> <li>Understand data quality and known limitations</li> </ol> <ol> <li>Document dataset versions and access dates</li> <li>Include proper citations in publications</li> <li>Note any preprocessing applied to the data</li> <li>Share your analysis code when possible</li> </ol>"},{"location":"gee/gee_catalog/#advanced-catalog-features","title":"Advanced Catalog Features","text":""},{"location":"gee/gee_catalog/#programmatic-access","title":"Programmatic Access","text":"<p>For power users and automated workflows, the catalog can be accessed programmatically:</p> <ul> <li>Earth Engine API - Direct dataset access through code</li> <li>Asset Management - Import and manage your own datasets</li> <li>Shared Assets - Access community-contributed datasets</li> <li>Apps Integration - Use catalog data in Earth Engine Apps</li> </ul>"},{"location":"gee/gee_community_catalog/","title":"Awesome GEE Community Catalog Community-Driven Geospatial Datasets","text":"<p>Community-Powered Data Commons</p> <p>Community-sourced datasets \u2022 Open licenses \u2022 Ready-to-use Earth Engine assets \u2022 Preprocessed for immediate analysis</p> <p>The Awesome GEE Community Catalog represents a grassroots effort to democratize access to geospatial data. This community-driven repository addresses a critical gap: research datasets that often require extensive preprocessing before they can be used in Google Earth Engine. By providing ready-to-use Earth Engine assets, the catalog eliminates barriers and accelerates research workflows.</p> <p> </p> Awesome GEE Community Catalog - Bridging data gaps through community collaboration <p>Project Origins</p> <p>\ud83d\udcd6 Learn More: Community Datasets: Data Commons in Google Earth Engine</p> <p>The project emerged from the recognition that many valuable research datasets remained inaccessible due to preprocessing complexity, licensing confusion, or format incompatibilities.</p>"},{"location":"gee/gee_community_catalog/#why-community-catalog-matters","title":"Why Community Catalog Matters","text":""},{"location":"gee/gee_community_catalog/#addressing-real-challenges","title":"Addressing Real Challenges","text":"Data AccessibilityOpen ScienceResearch Acceleration <ul> <li>Preprocessing Barriers - Complex datasets made analysis-ready</li> <li>Format Standardization - Consistent Earth Engine asset formats</li> <li>Documentation - Clear metadata and usage examples</li> <li>Quality Assurance - Community-validated datasets</li> </ul> <ul> <li>Open Licenses - Transparent usage rights</li> <li>Reproducible Research - Standardized data access</li> <li>Community Validation - Peer-reviewed quality</li> <li>Knowledge Sharing - Collaborative improvement</li> </ul> <ul> <li>Immediate Access - Skip time-consuming preprocessing</li> <li>Tested Examples - Working code for quick starts</li> <li>Domain Organization - Intuitive dataset discovery</li> <li>Regular Updates - Community-maintained freshness</li> </ul>"},{"location":"gee/gee_community_catalog/#dataset-categories","title":"Dataset Categories","text":"<p>The catalog organizes datasets into thematic domains for intuitive discovery:</p>"},{"location":"gee/gee_community_catalog/#core-domains","title":"Core Domains","text":"Domain Focus Areas Example Datasets \ud83c\udfd9\ufe0f Population &amp; Socioeconomics Demographics, economic indicators, urban development Population grids, poverty maps, economic activity \ud83c\udf31 Agriculture &amp; Food Security Crop mapping, yield prediction, food systems Crop type maps, agricultural statistics, food prices \ud83c\udf0a Hydrology &amp; Water Resources Water availability, quality, infrastructure River networks, water bodies, groundwater data \ud83d\udd25 Disasters &amp; Hazards Risk assessment, emergency response Flood maps, fire perimeters, vulnerability indices \ud83c\udf3f Ecology &amp; Biodiversity Species distribution, habitat mapping Protected areas, species occurrence, ecosystem maps \ud83c\udfd7\ufe0f Infrastructure &amp; Transportation Built environment, connectivity Road networks, building footprints, facility locations"},{"location":"gee/gee_community_catalog/#specialized-collections","title":"Specialized Collections","text":"<p>Unique Dataset Highlights</p> <p>\ud83d\udef0\ufe0f Radar &amp; SAR Data - Preprocessed radar datasets for advanced analysis</p> <p>\ud83d\udcca Statistical Surfaces - Gridded statistical data from national surveys</p> <p>\ud83d\uddfa\ufe0f Administrative Boundaries - High-quality boundary datasets for multiple countries</p> <p>\u23f0 Historical Archives - Digitized historical maps and imagery</p>"},{"location":"gee/gee_community_catalog/#navigating-the-catalog","title":"Navigating the Catalog","text":""},{"location":"gee/gee_community_catalog/#discovery-methods","title":"Discovery Methods","text":"Exploring datasets by thematic domains - expand categories to discover relevant datasets <p>Finding the Right Dataset</p> <p>\ud83d\udcc2 Browse by Domain - Expand thematic categories to explore related datasets</p> <p>\ud83d\udd0d Keyword Search - Use the search bar for specific terms, tags, or dataset names</p> <p>\ud83c\udff7\ufe0f Tag Filtering - Combine multiple tags to narrow your search</p> <p>\ud83d\udccb Asset Lists - Each dataset page shows all available Earth Engine assets</p> <p> </p> Using the search functionality to quickly find datasets by name, keywords, or tags"},{"location":"gee/gee_community_catalog/#dataset-pages","title":"Dataset Pages","text":"<p>Each dataset in the catalog provides comprehensive information:</p> Essential InformationTechnical DetailsUsage Resources <ul> <li>Dataset Description - Comprehensive overview and methodology</li> <li>Spatial/Temporal Coverage - Geographic extent and time range</li> <li>Data Source - Original data providers and processing details</li> <li>Licensing Information - Clear usage rights and restrictions</li> </ul> <ul> <li>Earth Engine Asset IDs - Direct links to data assets</li> <li>Coordinate Reference System - Projection and datum information</li> <li>Resolution Specifications - Spatial and temporal resolution</li> <li>Data Quality Notes - Known limitations and considerations</li> </ul> <ul> <li>Example Code - Working JavaScript and Python examples</li> <li>Visualization Parameters - Pre-configured display settings</li> <li>Processing Tips - Best practices for analysis</li> <li>Citation Guidelines - Proper attribution requirements</li> </ul>"},{"location":"gee/gee_community_catalog/#integration-with-earth-engine","title":"Integration with Earth Engine","text":""},{"location":"gee/gee_community_catalog/#code-editor-repository","title":"Code Editor Repository","text":"Adding the community catalog repository to your Earth Engine Code Editor for easy access <p>Easy Integration</p> <p>\ud83d\udd17 Add Repository: Include the entire catalog as a Code Editor repository</p> <p>\ud83d\udcf1 Quick Access: Browse examples directly in your development environment</p> <p>\ud83d\udd04 Automatic Updates: Stay current with new datasets and improved examples</p> <p>\ud83d\udcbb Ready-to-Run Code: Copy and modify examples for your specific needs</p>"},{"location":"gee/gee_community_catalog/#working-with-community-datasets","title":"Working with Community Datasets","text":"<pre><code>// Example: Loading a community dataset\nvar populationGrid = ee.Image('projects/sat-io/open-datasets/worldpop/ppp_2020_1km_Aggregated');\n\n// Apply filtering and visualization\nvar visualization = {\n  min: 0,\n  max: 1000,\n  palette: ['white', 'yellow', 'orange', 'red', 'purple']\n};\n\nMap.addLayer(populationGrid, visualization, 'Population Density 2020');\n</code></pre> <p>Best Practices for Community Data</p> <p>\u26a1 Check Asset Status - Verify asset availability before deployment</p> <p>\ud83d\udcd6 Read Documentation - Understand processing methodology and limitations</p> <p>\ud83c\udff7\ufe0f Use Proper Citations - Credit original data providers and catalog contributors</p> <p>\ud83d\udd04 Stay Updated - Monitor for dataset updates and improvements</p>"},{"location":"gee/gee_community_catalog/#contributing-to-the-catalog","title":"Contributing to the Catalog","text":""},{"location":"gee/gee_community_catalog/#how-to-get-involved","title":"How to Get Involved","text":"<p>Community Participation</p> <p>The catalog thrives on community contributions. Here's how you can help:</p> Data ContributorsTechnical ContributorsCommunity Support <ul> <li>Submit Datasets - Share your preprocessed research datasets</li> <li>Improve Documentation - Enhance existing dataset descriptions</li> <li>Validate Quality - Test and verify dataset accuracy</li> <li>Update Examples - Improve or add new code examples</li> </ul> <ul> <li>Code Development - Enhance catalog infrastructure</li> <li>Bug Reporting - Identify and report issues</li> <li>Feature Requests - Suggest new functionality</li> <li>Testing - Validate new features and datasets</li> </ul> <ul> <li>User Feedback - Share experiences and suggestions</li> <li>Documentation - Improve tutorials and guides</li> <li>Outreach - Promote catalog usage in your networks</li> <li>Training - Help others learn to use community datasets</li> </ul>"},{"location":"gee/gee_community_catalog/#contribution-guidelines","title":"Contribution Guidelines","text":"<p>Quality Standards</p> <p>\ud83d\udccb Documentation Required - Complete metadata and usage examples</p> <p>\u2696\ufe0f Open Licensing - Datasets must use open, clearly specified licenses</p> <p>\ud83d\udd0d Quality Assurance - Data must be validated and tested</p> <p>\ud83e\udd1d Community Review - Submissions undergo peer review process</p>"},{"location":"gee/gee_community_catalog/#impact-and-recognition","title":"Impact and Recognition","text":""},{"location":"gee/gee_community_catalog/#growing-ecosystem","title":"Growing Ecosystem","text":"<p>The Awesome GEE Community Catalog has become an essential resource for the Earth Engine community:</p> <ul> <li>\ud83c\udf0d Global Reach - Used by researchers and practitioners worldwide</li> <li>\ud83d\udcc8 Growing Collection - Continuously expanding dataset portfolio</li> <li>\ud83c\udf93 Educational Value - Supporting teaching and learning initiatives</li> <li>\ud83d\udd2c Research Enablement - Accelerating scientific discoveries</li> </ul>"},{"location":"gee/gee_community_catalog/#academic-citation","title":"Academic Citation","text":"<p>When using datasets from the community catalog, please cite both the original data source and the catalog project:</p> <p>Citation Format</p> <p>Roy, S., &amp; Swetnam, T. (2024). samapriya/awesome-gee-community-datasets: Community Catalog (2.6.0). Zenodo. https://doi.org/10.5281/zenodo.11118613</p>"},{"location":"gee/gee_community_catalog/#getting-started","title":"Getting Started","text":""},{"location":"gee/gee_community_catalog/#quick-start-guide","title":"Quick Start Guide","text":"<ol> <li>\ud83c\udf10 Explore the Catalog - Visit gee-community-catalog.org</li> <li>\ud83d\udd0d Find Your Data - Use search or browse by domain</li> <li>\ud83d\udcd6 Read Documentation - Understand dataset specifications and limitations</li> <li>\ud83d\udcbb Try Examples - Run provided code in Earth Engine Code Editor</li> <li>\ud83d\ude80 Build Your Analysis - Adapt examples for your research needs</li> </ol>"},{"location":"gee/gee_community_catalog/#next-steps","title":"Next Steps","text":"<p>Maximize Your Impact</p> <p>\ud83e\udd1d Join the Community - Engage with other users and contributors</p> <p>\ud83d\udce2 Share Your Work - Showcase analyses built with community datasets</p> <p>\ud83d\udd04 Give Feedback - Help improve datasets and documentation</p> <p>\ud83d\udcda Stay Informed - Follow updates and new dataset announcements</p> <p>The Awesome GEE Community Catalog represents the power of collaborative science - join the community and help democratize access to geospatial data.</p>"},{"location":"gee/gee_overview/","title":"Google Earth Engine Browser Based Remote Sensing","text":"<p>Key Benefits</p> <p>90+ petabytes of open geospatial data \u2022 Cloud-based processing \u2022 JavaScript &amp; Python SDKs \u2022 Scalable analysis from local to global</p> <p>Google Earth Engine brings the power of remote sensing analysis into the browser. With just an internet connection, anyone can tap into this cloud-based platform to process and query massive geospatial datasets.</p> <p>Earth Engine levels the playing field for working with satellite imagery and raster data at scale. It hosts one of the largest repositories of open geospatial assets (over 90+ petabytes), while ingesting new data continuously. Users can perform analysis across these indexed layers, going from local to global, using JavaScript and Python SDKs. The cloud infrastructure allows researchers to iterate rapidly and run computations on petabytes of data. Results are shareable, published outputs that ensure reproducibility. Earth Engine simplifies the process from data access to analysis to dissemination.</p> <p> </p> What is Google Earth Engine <p>Why Earth Engine Matters</p> <p>By moving most of the processing into the cloud, Earth Engine makes remote sensing more accessible. Users now wield these capabilities through any web browser. This democratization of geospatial analytics opens up new possibilities for applications across numerous domains.</p>"},{"location":"gee/gee_overview/#capabilities-and-limitations","title":"Capabilities and Limitations","text":"<p>Understanding what Google Earth Engine excels at\u2014and where it has boundaries\u2014helps you make the most of this powerful platform.</p>"},{"location":"gee/gee_overview/#what-earth-engine-does-best","title":"What Earth Engine Does Best","text":"Image ProcessingVector ProcessingTerrain AnalysisTime SeriesAdvanced Analytics <ul> <li>Map Algebra, Kernels and Convolutions</li> <li>Spectral Unmixing, Pan-sharpening</li> <li>Gap Filling, Data Fusion</li> </ul> <ul> <li>Zonal Statistics, Spatial Joins</li> <li>Spatial Query operations</li> </ul> <ul> <li>Slope, Aspect, Hillshades</li> <li>Hill Shadow Analysis</li> </ul> <ul> <li>Extract Time-Series, Trend Analysis</li> <li>Time-Series Smoothing</li> <li>Temporal Segmentation</li> </ul> <ul> <li>Object-based Image Analysis (GLCM, Texture, Hotspots)</li> <li>Change Detection (Spectral Distance, Change Classification)</li> <li>Machine Learning (Supervised/Unsupervised Classification, PCA)</li> <li>Deep Learning (DNN, Object Detection via TensorFlow)</li> </ul>"},{"location":"gee/gee_overview/#current-limitations","title":"Current Limitations","text":"<p>Not supported in Earth Engine</p> <ul> <li>Cartographic Outputs - No traditional map layout tools</li> <li>3D Visualization - Limited to 2D analysis and display</li> <li>Hydrological Modeling - No rainfall-runoff or watershed tools</li> <li>Photogrammetry - No orthorectification or point cloud processing</li> <li>LiDAR Processing - No specialized LiDAR analysis tools</li> <li>SAR Interferometry - Limited SAR analysis capabilities</li> </ul>"},{"location":"gee/gee_overview/#additional-resources","title":"Additional Resources","text":"<p>Learn More</p> <p>\ud83d\udcc4 Research Paper: Google Earth Engine: Planetary-scale geospatial analysis for everyone</p> <p>\ud83d\udcda Comprehensive Guide: Cloud-Based Remote Sensing with Google Earth Engine: Fundamentals and Applications - Open source book by the GEE community</p> <p> </p> Snippet from the Google Earth Engine main paper. Source"},{"location":"setup/getting_started/","title":"Getting Started","text":"<p>This guide will help you prepare for the workshop by setting up your Google Earth Engine account and understanding the prerequisites.</p>"},{"location":"setup/getting_started/#prerequisites","title":"Prerequisites","text":"<p>Participants should have basic familiarity with Google Earth Engine and experience coding in a scientific language (e.g., Python, R, JavaScript) is recommended. Participants must sign up for a Google Earth Engine account and set up a cloud project to follow along with the live-coding exercises.</p>"},{"location":"setup/getting_started/#setup-instructions","title":"Setup Instructions","text":"<p>To follow along during the workshop, or to run through the tutorials contained within the repository using the Google Earth Engine Code Editor, the following steps are required.</p>"},{"location":"setup/getting_started/#1-google-earth-engine-account","title":"1. Google Earth Engine Account","text":"<p>Create a Noncommercial Google Earth Engine account (if you don't already have one) at https://earthengine.google.com/noncommercial/</p>"},{"location":"setup/getting_started/#2-google-earth-engine-project","title":"2. Google Earth Engine Project","text":"<p>Once your EE account is set up, you will need to create a Google Cloud project. This is required to run the live-coding exercises. You can follow the instructions in the Google Earth Engine access guide to set up your project.</p>"},{"location":"setup/getting_started/#3-hardware-requirements","title":"3. Hardware Requirements","text":"<p>Following along with the exercises requires a laptop or tablet with a stable internet connection.</p>"},{"location":"setup/getting_started/#4-code-repository","title":"4. Code Repository","text":"<p>Once we have the earth engine account set up, you can click on this link to accept the Earth Engine repository which contains the codes for todays class</p>"},{"location":"setup/getting_started/#next-steps","title":"Next Steps","text":"<p>Once you've completed the setup above, you'll be ready to participate in the workshop exercises. Make sure to test your Google Earth Engine access by logging into the Earth Engine Code Editor before the workshop begins.</p> <p>If you encounter any issues during setup, please reach out to the workshop organizers for assistance.</p>"},{"location":"tutorials/01_intro_to_neon_aop_gee_datasets/","title":"Introduction to AOP Public Datasets in Google Earth Engine (GEE)","text":"\ud83d\udccb Tutorial Details\ud83c\udff7\ufe0f Topics &amp; Data <ul> <li>Duration: 30 minutes</li> <li>Level: Intermediate</li> <li>Authors: Bridget Hass, John Musinsky</li> <li>Contributors: Tristan Goulden, Lukas Straube</li> </ul> <ul> <li>Topics: lidar, hyperspectral, camera, remote-sensing</li> <li>Data Products: DP3.30006.001, DP3.30006.002, DP3.30010.001, DP3.30015.001, DP3.30024.001</li> </ul>"},{"location":"tutorials/01_intro_to_neon_aop_gee_datasets/#neon-aop-data-in-google-earth-engine","title":"NEON AOP Data in Google Earth Engine","text":"<p>Google Earth Engine (GEE) is a free and powerful cloud-computing platform for carrying out remote sensing and geospatial data analysis. In this tutorial, we introduce you to the NEON AOP datasets that have been added to Google Earth Engine as Publisher Datasets.</p> <p></p> <p>NEON is planning to add the full archive of AOP L3 Surface Bidirectional Reflectance, LiDAR Elevation, Ecosystem Structure, and High-resolution orthorectified camera imagery. Since the L3 Surface Directional Reflectance is being replaced by the bidirectional (Bidirectional Reflectance Distribution Function (BRDF) and topographic corrected) reflectance as that becomes available, we are only adding directional reflectance data to GEE upon request.</p> <p>Data Availability Update</p> <p>As of July 2025, bidirectional data is only available for AOP data collected between 2022-2024, but re-processing of older AOP data (2013-2021) will begin in mid-2025. Please see the tutorial Introduction to Bidirectional Hyperspectral Reflectance Data in Python for more information on the differences between the directional and bidirectional reflectance data products.</p> <p>It will take time for the full archive of AOP data to be added to GEE, but NEON has been ramping up data additions starting in Fall 2024. This tutorial shows you how to find which data are currently available. If there are certain NEON sites and years of data you would like to see added to Google Earth Engine sooner, use the NEON Contact Us form to request this, and include \"Google Earth Engine Remote Sensing Data\" in the text.</p>"},{"location":"tutorials/01_intro_to_neon_aop_gee_datasets/#objectives","title":"Objectives","text":"<p>After completing this activity, you will become familiar with:</p> <ul> <li>Google Earth Engine (GEE)</li> <li>NEON AOP Image Collections in GEE</li> </ul> <p>And you will be able to:</p> <ul> <li>Write and run basic JavaScript code in the GEE Code Editor</li> <li>Discover which NEON AOP datasets are available in GEE</li> <li>Explore the NEON AOP GEE Image Collections</li> <li>Plot an RGB image of a reflectance dataset</li> <li>Compare bidirectional and directional reflectance datasets</li> </ul>"},{"location":"tutorials/01_intro_to_neon_aop_gee_datasets/#requirements","title":"Requirements","text":"<ul> <li>A Google or gmail (@gmail.com) account</li> <li>An Earth Engine account. You can sign up for an Earth Engine account here: https://earthengine.google.com/new_signup/. Click on \"Register a Noncommercial or Commercial Cloud Project\", and on the next prompt select \"Unpaid Usage\" and select the Project Type to create a free non-commercial account. For more information, refer to Noncommercial Earth Engine</li> <li>A Google Cloud Project. See Set up your Earth Engine enabled Cloud Project</li> <li>A basic understanding of the GEE Code Editor and the GEE JavaScript API</li> </ul>"},{"location":"tutorials/01_intro_to_neon_aop_gee_datasets/#additional-resources","title":"Additional Resources","text":"<p>If this is your first time using GEE, we recommend starting on the Google Developers website, and working through some of the introductory tutorials. The links below are good places to start.</p> <ul> <li>Get Started with Earth-Engine</li> <li>GEE JavaScript Tutorial</li> </ul>"},{"location":"tutorials/01_intro_to_neon_aop_gee_datasets/#aop-gee-data-access","title":"AOP GEE Data Access","text":"<p>AOP has currently added a subset of AOP Level 3 (tiled) data products at over 50 NEON sites spanning 10 years on GEE (as of Jan 2025). The NEON data products that have been made available on GEE can be currently be found on the GEE Datasets page, if you search for \"NEON\" as follows:</p> <p></p> <p>In the code editor, NEON datasets can be accessed through the <code>projects/neon-prod-earthengine</code> folder with an appended suffix of the Acronym and Revision Number, shown in the table below. For example, the Surface Directional Reflectance can be found under the path <code>projects/neon-prod-earthengine/assets/HSI_REFL/001</code>. The table below summarizes the GEE and NEON Data Portal IDs for each data product, and can be used as a reference for reading in AOP GEE datasets.</p> GEE Acronym/Revision Data Product Name Data Product ID HSI_REFL/001 Surface Directional Reflectance DP3.30006.001 HSI_REFL/002 Surface Bidirectional Reflectance DP3.30006.002 RGB/001 Red Green Blue (Camera Imagery) DP3.30010.001 DEM/001 Digital Elevation Model (DSM/DTM) DP3.30024.001 CHM/001 Ecosystem Structure (Canopy Height Model; CHM) DP3.30015.001"},{"location":"tutorials/01_intro_to_neon_aop_gee_datasets/#get-started-with-google-earth-engine","title":"Get Started with Google Earth Engine","text":"<p>Once you have set up your Google Earth Engine account you can navigate to the Earth Engine Code Editor. The diagram below, from the Earth-Engine Playground, shows the main components of the code editor. If you have used other programming languages such as R, Python, or Matlab, this should look fairly similar to other Integrated Development Environments (IDEs) you may have worked with. The main difference is that this has an interactive map at the bottom, similar to Google Maps and Google Earth.</p> <p></p> <p>We encourage you to play around with the interactive map, or explore the ee documentation, linked above, to gain familiarity with the various features.</p>"},{"location":"tutorials/01_intro_to_neon_aop_gee_datasets/#read-aop-data-collections-into-gee","title":"Read AOP Data Collections into GEE","text":"<p>AOP data can currently be accessed through GEE through the <code>projects/neon-prod-earthengine/assets/</code> folder. In the remainder of this lesson, we will look at the five available AOP datasets, or <code>ImageCollections</code>.</p> <p>An ImageCollection is simply a group of images. To find publicly available datasets (primarily satellite data), you can explore the Earth Engine Data Catalog. The following steps will walk you through how to read in AOP Image Collections in the Code Editor.</p> <p>In your code editor, copy and run the following lines of code to create 5 <code>ImageCollection</code> variables containing the Surface Directional Reflectance (HSI_REFL/001), Surface Bidirectional Reflectance (HSI_REFL/002), Camera Imagery (RGB), Canopy Height Model (CHM), and Digital Elevation Model (DEM) raster data sets.</p> <pre><code>//read in the AOP image collections as variables\n\nvar refl001 = ee.ImageCollection('projects/neon-prod-earthengine/assets/HSI_REFL/001')\n\nvar refl002 = ee.ImageCollection('projects/neon-prod-earthengine/assets/HSI_REFL/002')\n\nvar rgb = ee.ImageCollection('projects/neon-prod-earthengine/assets/RGB/001')\n\nvar chm = ee.ImageCollection('projects/neon-prod-earthengine/assets/CHM/001')\n\nvar dem = ee.ImageCollection('projects/neon-prod-earthengine/assets/DEM/001')\n</code></pre> <p>Working in the Code Editor</p> <ul> <li>In the left panel of the code editor, there is a Docs tab which includes API documentation on built in functions, showing the expected input arguments. We encourage you to refer to this documentation, as well as the GEE JavaScript Tutorial to familiarize yourself with GEE and the JavaScript programming language.</li> <li>If you have an error in your code, a red error message will show up in the Console (in the right panel), which tells you the line that failed.</li> <li>Save your code frequently! If you try to leave your code while it is unsaved, you will be prompted that there are unsaved changes in the editor.</li> </ul> <p>When you Run the code above (by clicking on the Run above the code editor), you will notice that the lines of code become underlined in red, the same as you would see for a spelling error in most text editors. If you hover over each of the lines of codes, you will see a message pop up that prompts you to Convert the variable into an import record.</p> <p></p> <p>If you click <code>Convert</code>, the line of code will disappear and the variable will be imported into your session directly, and will show up at the top of the code editor. Go ahead and convert the variables for all three lines of code, so you should see the following.</p> <p>Maintaining Reproducibility</p> <p>If you type <code>Ctrl-z</code>, you can re-generate the line of code, and the variable will still show up in the imported variables at the top of the editor. It is recommended to retain the code that reads in each variable, for reproducibility. If you don't do this, and wish to share this code with someone else, or run the code outside of your current code editor, the imported variables will not be saved and any subsequent code referring to this variable will result in an error message.</p> <p></p> <p>Note that each of these imported variables can now be expanded, using the arrow to the left of each. These variables now show associated information including type, id, and version.</p> <p>Information about the image collections can also be found in a slightly more user-friendly format if you click on the blue link, eg. <code>projects/neon-prod-earthengine/CHM/001</code>. Below we'll show the window that pops-up when you click on the CHM link. We encourage you to explore all of the AOP datasets similarly.</p> <p></p> <p>Dataset Information</p> <p>You can also search for the NEON AOP image collections through the search bar on the Earth Engine Data Catalog webpage. The dataset page also contains all the information about the data product, eg. NEON Canopy Height Model (CHM).</p> <p>The end of the description includes a link to the Data Product landing page on the NEON Data Portal, as well as the Quick Start Guide, which includes links to all the documentation pertaining to this NEON data product, including the Algorithm Theoretical Basis Documents (ATBDs). Click on the other tabs to explore more about this data product. These tabs include <code>DESCRIPTION</code>, <code>BANDS</code>, <code>IMAGE PROPERTIES</code>, <code>TERMS OF USE</code>, AND <code>CITATIONS</code>.</p> <p>Alternative Search Method</p> <p>You can also search for NEON data products through Code Editor by typing \"NEON\" in the search bar as shown below:</p> <p></p>"},{"location":"tutorials/01_intro_to_neon_aop_gee_datasets/#aop-gee-data-availability","title":"AOP GEE Data Availability","text":"<p>Since we are adding AOP data to GEE on a rolling basis, the first thing you may want to do after reading in the image collections is to determine which datasets are currently available on GEE. A quick way to do this is shown below:</p> <pre><code>// list all available images in the NEON Surface Directional Reflectance Image Collection:\nprint('NEON Images in the Directional Reflectance Collection',\n      refl001.aggregate_array('system:index'))\n\n// list all available images in the NEON Surface Bidirectional Reflectance Image Collection:\nprint('NEON Images in the Bidirectional Reflectance Collection',\n      refl002.aggregate_array('system:index'))\n\n// list all available images in the NEON DEM image collection:\nprint('NEON Images in the DEM Collection',\n      dem.aggregate_array('system:index'))\n\n// list all available images in the NEON CHM image collection:\nprint('NEON Images in the CHM Collection',\n      chm.aggregate_array('system:index'))\n\n// list all available images in the NEON CHM image collection:\nprint('NEON Images in the RGB Camera Collection',\n      rgb.aggregate_array('system:index'))\n</code></pre> <p>In the Console tab to the right of the code, you will see a list of all available images. Expand each List to see the data available for each Image Collection. The names of the all the images follow the format <code>YEAR_SITE_#</code>, so you can identify the site and year of data this way. The number at the end is the Visit #; AOP typically visits each site 3 out of every 5 years, so the visit number indicates the cumulative number of times AOP has visited that site. Occasionally, AOP may re-visit a site twice in the same year.</p> <p></p>"},{"location":"tutorials/01_intro_to_neon_aop_gee_datasets/#filter-by-image-properties","title":"Filter by Image Properties","text":"<p>Next, we can explore some filtering options to pull out individual images from an Image Collection. In the example shown below, we can filter by the date (<code>.filterDate</code>) by providing a date range, and filter by other properties, such as the NEON site code, using <code>.filterMetadata</code>. For this example we'll pull in an image from the NEON site Lyndon B. Johnson National Grassland NEON (CLBJ).</p> <pre><code>// read in a single reflectance image at the NEON site CLBJ in 2021\nvar refl001_CLBJ_2021 = refl001\n  .filterDate('2021-01-01', '2021-12-31') // filter by date - 2021\n  .filterMetadata('NEON_SITE', 'equals', 'CLBJ') // filter by site\n  .first(); // select the first one to pull out a single image\n</code></pre>"},{"location":"tutorials/01_intro_to_neon_aop_gee_datasets/#explore-image-properties","title":"Explore Image Properties","text":"<p>Next let's take a look at the Image Properties.</p> <pre><code>// look at the image properties\nvar clbj2021_refl_properties = refl001_CLBJ_2021.toDictionary()\nprint('CLBJ 2021 Directional Reflectance Properties:', clbj2021_refl_properties)\n</code></pre> <p>Look in the Console for the properties, you can expand by clicking on the arrow to the left of the <code>Object (438 properties)</code>. Here you can see some metadata about this image. Scroll down and you'll get to a number of properties starting with <code>WL_FWHM_B###</code>. These are the WaveLength (WL) and Full Width Half Max (FWHM) values, in nanometers, corresponding to each band (Bands 001 - 426). You may wish to refer to this wavelength information to determine which bands you wish to display, eg. if you want to show a false color image instead of a true color (RGB) image. For a full description of what each of the Image Properties mean, you can look at the <code>IMAGE PROPERTIES</code> tab as explained in the previous section, or find it in the Earth Engine Data Catalog.</p> <p></p>"},{"location":"tutorials/01_intro_to_neon_aop_gee_datasets/#determine-release-tag-information","title":"Determine Release Tag Information","text":"<p>When working with NEON data, whether downloaded from the Data Portal or on GEE, we always recommend checking whether the data are Provisional or Released, and the release tag of the data. On GEE, this information is included in the image properties <code>PROVISIONAL_RELEASED</code> and <code>RELEASE_YEAR</code>. If the data is released, the property <code>RELEASE_YEAR</code> will display the year of the release. The code chunk below shows how to display the release information for the CLBJ 2021 directional reflectance data.</p> <pre><code>// determine the release information for this image\nvar clbj2021_release_status = clbj2021_refl_properties.select(['PROVISIONAL_RELEASED']);\nprint('CLBJ 2021 Directional Reflectance Release Status:', clbj2021_release_status)\n\nvar clbj2021_release_year = clbj2021_refl_properties.select(['RELEASE_YEAR']);\nprint('CLBJ 2021 Directional Reflectance Release Year:', clbj2021_release_year)\n</code></pre> <p>In this example, the data is part of <code>RELEASE-2024</code>.</p> <p>NEON Data Releases</p> <p>For more information on NEON releases, refer to the NEON Data Product Revisions and Releases page. There is a short period each year in January where AOP data on the NEON Data Portal may be in flux in preparation for an upcoming data release (typically end of January). GEE datasets are planned to be kept up to date with the current release, however there may be a lag period between the annual release and data updates on GEE. Data on GEE should be updated to match the current release by the end of February each year. For current information around the release status and data quality issue notices, you can follow NEON Data Notifications.</p>"},{"location":"tutorials/01_intro_to_neon_aop_gee_datasets/#plot-a-true-color-image","title":"Plot a True Color Image","text":"<p>Finally, let's plot a true color image (red-green-blue or RGB composite) of the reflectance data that we've read into the variable <code>refl001_CBLJ_2021</code>. To do this, first we pull out the RGB bands, set visualization parameters, center the map over the site, and then add the map using <code>Map.addLayer</code>. There are a couple ways you can center the Map to the location you want. One is to use <code>Map.centerObject</code> and you can provide the image you want to center; otherwise you can specify the latitude and longitude, shown commented-out in the code chunk below.</p> <pre><code>// pull out the red, green, and blue bands\nvar refl001_CLBJ_2021_RGB = refl001_CLBJ_2021.select(['B053', 'B035', 'B019']);\n\n// set visualization parameters\nvar refl_rgb_vis = {min: 0, max: 1260, gamma: 0.8};\n\n// use centerObject to center on the reflectance data, 13 is the zoom level\nMap.centerObject(refl001_CLBJ_2021, 13)\n\n// alternatively you could specify the lat / lon of the site, set zoom to 13\n// you can find the field site lat/lon here https://www.neonscience.org/field-sites/clbj\n// Map.setCenter(-97.57, 33.40, 13);\n\n// add this RGB layer to the Map and give it a title\nMap.addLayer(refl001_CLBJ_2021_RGB, refl_rgb_vis, 'CLBJ 2021 Directional Reflectance RGB');\n</code></pre> <p>When you run the code you should now see the true color image on the map! You can zoom in and out and explore some of the other interactive options on your own.</p> <p></p>"},{"location":"tutorials/01_intro_to_neon_aop_gee_datasets/#compare-directional-and-bidirectional-reflectance","title":"Compare Directional and Bidirectional Reflectance","text":"<p>Lastly, let's also look at a bidirectional data product at the same site, and you can explore the differences between the directional and bidirectional reflectance. We will also display the release information for this data.</p> <pre><code>// read in a bidirectional reflectance image at the NEON site CLBJ in 2022\nvar refl002_CLBJ_2022 = refl002\n  .filterDate('2022-01-01', '2022-12-31') // filter by date - 2022\n  .filterMetadata('NEON_SITE', 'equals', 'CLBJ') // filter by site\n  .first(); // select the first one to pull out a single image\n\n// read the properties into a variable\nvar clbj2022_refl_properties = refl002_CLBJ_2022.toDictionary()\n\n// determine the release information for this BRDF-corrected image\nvar clbj2022_release_status = clbj2022_refl_properties.select(['PROVISIONAL_RELEASED']);\nprint('CLBJ 2022 Bidirectional Reflectance Release Status:', clbj2022_release_status)\n\n// if you try to read in the release year, it will throw an error\n// since this data product is still PROVISIONAL, there is no release year\n// comment out these lines below to remove\nvar clbj2022_release_year = clbj2022_refl_properties.select(['RELEASE_YEAR']);\nprint('CLBJ 2022 Bidirectional Reflectance Release Year:', clbj2022_release_year)\n\n// pull out the red, green, and blue bands\nvar refl002_CLBJ_2022_RGB = refl002_CLBJ_2022.select(['B053', 'B035', 'B019']);\n\n// add this RGB layer to the Map and give it a title\nMap.addLayer(refl002_CLBJ_2022_RGB, refl_rgb_vis, 'CLBJ 2022 Bidirectional Reflectance RGB');\n</code></pre> <p></p> <p>Handling Errors</p> <p>If your code has any errors they will display in the Console tab in red. In this example, we tried to print out a property that does not exist because the data is Provisional, so there is no <code>RELEASE_YEAR</code>. You can comment out the lines of code starting with <code>var clbj2022_release_year</code> to prevent the error from displaying. If your code is not running as expected, errors displayed in the Console can be helpful for troubleshooting, as it will tell you how and where your code failed. Print statements throughout the code can also be helpful.</p> <p>Note that bidirectional reflectance data will remain provisional in 2025, since it is a new data product (as of 2024), and is planned to be incorporated into RELEASE-2026.</p> <p>You can toggle between the two layers by selecting the \"Layers\" tab in the upper right corner of the Map window. Check and uncheck the two layers (2021 and 2022) to see the differences. You can also use the slider to the right of the layer name to make one layer partially transparent. What observations can you make about these two datasets?</p> <p></p> <p>The BRDF and topographic corrections typically visibly improve striping (or BRDF effects) between adjacent flightlines, as we can see with these datasets at CLBJ, where the 2022 bidirectional reflectance (left) looks much more seamless than the 2021 directional reflectance data (right), which has some visible vertical artifacts. For most NEON sites, the flight lines are oriented N-S so the stripes in the directional reflectance data will be vertical, but there are a few sites with slightly different flight plans.</p>"},{"location":"tutorials/01_intro_to_neon_aop_gee_datasets/#summary","title":"Summary","text":"<p>You did it! You now have a basic understanding of the GEE Code Editor and its different components. You have also learned how to read a NEON AOP <code>ImageCollection</code> into a variable, import the variable into your session, and navigate through the ImageCollection Asset details to display information about the collection. You learned to read in an individual reflectance image, explore the image properties, and display a map of a true color image (RGB composite). And finally, you explored some of the differences between the directional and bidirectional (BRDF- and topographic corrected) reflectance data products at the site CLBJ.</p> <p>It doesn't seem like we've done much so far, but this is a already great achievement! With just a few lines of code, you can import an entire AOP hyperspectral dataset, which in most other coding environments, is more involved. One of the major challenges to working with AOP reflectance data is its large data volume, which typically requires high-performance computing environments to read in the data, visualize, and analyze it. There are also limited open-source tools for working with hyperspectral data; many of the established software suites require proprietary (and often expensive) licenses. In this lesson, with minimal code, we have loaded spectral, lidar, and camera data covering an entire AOP site, and are ready to start exploring and analyzing the data in a free geospatial cloud-computing platform.</p>"},{"location":"tutorials/01_intro_to_neon_aop_gee_datasets/#get-lesson-code","title":"Get Lesson Code","text":"<p>Intro to AOP GEE Image Collections</p>"},{"location":"tutorials/02_neon_reflectance_weather_qa/","title":"Reflectance Pre-processing: Masking Out Bad Weather Data in GEE","text":"\ud83d\udccb Tutorial Details\ud83c\udff7\ufe0f Topics &amp; Data <ul> <li>Duration: 30 minutes</li> <li>Level: Intermediate</li> <li>Authors: Bridget M. Hass, John Musinsky</li> <li>Contributors: Tristan Goulden</li> </ul> <ul> <li>Topics: hyperspectral, remote-sensing</li> <li>Data Product: DP3.30006.001</li> </ul>"},{"location":"tutorials/02_neon_reflectance_weather_qa/#introduction","title":"Introduction","text":"<p>Since reflectance data is generated from a passive energy source (the sun), data collected in cloudy sky conditions are not directly comparable to data collected in clear-sky conditions, as overhead clouds can obscure the incoming light source. AOP aims to collect data only in optimal (&lt;10% cloud-cover) weather conditions, but cannot always do so due to logistical constraints. The flight operators record the weather conditions during each flight, and this information is passed through to the final data product at the level of the flight line (as cloud conditions can change throughout the day).</p> <p>Cloud conditions are reported as: - Green (&lt;10% cloud cover) - Yellow (10-50% cloud cover) - Red (&gt;50% cloud cover)</p> <p>The figure below shows some examples of what the cloud conditions look like at different flights collected in the three different weather classes.</p> <p> Cloud cover percentage during AOP flights. Left: green (&lt;10%), Middle: yellow (10-50%), Right: red (&gt;50%).</p> <p>Airborne vs. Satellite Data</p> <p>There is an important distinction between airborne and satellite reflectance data. Satellite data is collected in all weather conditions, and the clouds are below the sensor, so algorithms can be generated to filter out cloudy pixels. With aerial data, we have more control over when the data are collected, to a degree. However, clouds may be present overhead, if it were deemed necessary to collect in sub-optimal weather conditions. AOP typically will only collect in \"red\" sky conditions if we are running out of time in a Domain and the weather isn't forecasted to improve.</p> <p>Since the clouds won't appear in the actual data, maintaining this record of cloud conditions is essential for properly understanding the data, and using it for change detection or other research applications. For a more direct comparison of reflectance values, we recommend only working with the clear-weather data. This lesson outlines how to do this in GEE.</p>"},{"location":"tutorials/02_neon_reflectance_weather_qa/#objectives","title":"Objectives","text":"<p>After completing this activity, you will be able to:</p> <ul> <li>Extract and plot the weather quality indicator band from the Surface Directional Reflectance dataset</li> <li>Mask reflectance data to pull out only clear-weather data for a given site</li> <li>Explore other QA bands included in the Reflectance data set</li> </ul>"},{"location":"tutorials/02_neon_reflectance_weather_qa/#requirements","title":"Requirements","text":"<ul> <li>Complete the following introductory AOP GEE tutorials:<ul> <li>Introduction to AOP Public Datasets in Google Earth Engine (GEE)</li> </ul> </li> <li>An understanding of hyperspectral data and AOP spectral data products. If this is your first time working with AOP hyperspectral data, we encourage you to start with:<ul> <li>Intro to Working with Hyperspectral Remote Sensing Data in R. You do not need to follow along with the code in those lessons, but at least read through to gain a better understanding of NEON's hyperspectral data product.</li> </ul> </li> </ul>"},{"location":"tutorials/02_neon_reflectance_weather_qa/#read-in-the-aop-surface-directional-reflectance-2019-dataset-at-soap","title":"Read in the AOP Surface Directional Reflectance 2019 Dataset at SOAP","text":"<p>For this exercise, we will read in directional reflectance data from the NEON site Soaproot Saddle (SOAP) collected in 2019:</p> <pre><code>// Filter image collection by date and site to pull out a single image\nvar soapSDR = ee.ImageCollection(\"projects/neon-prod-earthengine/assets/HSI_REFL/001\")\n  .filterDate('2019-01-01', '2019-12-31')\n  .filterMetadata('NEON_SITE', 'equals', 'SOAP')\n  .first();\n</code></pre>"},{"location":"tutorials/02_neon_reflectance_weather_qa/#display-the-qa-bands","title":"Display the QA Bands","text":"<p>From the previous lesson, recall that the reflectance images include 442 bands. Bands 0-425 are the data bands, which store the spectral reflectance values for each wavelength recorded by the NEON Imaging Spectrometer (NIS). The remaining bands (426-441) contain metadata and QA information that are important for understanding and properly interpreting the hyperspectral data. The data bands all follow the naming convention B001, B002, ..., B426, and the QA bands have more descriptive names that start with something other than the letter \"B\", so we can use that information to extract the QA bands.</p> <pre><code>// Pull out and display only the qa bands (these all start with something other than B)\n// '[^B].*' is a regular expression to pull out bands that don't start with B\nvar soapSDR_qa = soapSDR.select('[^B].*')\nprint('QA Bands',soapSDR_qa)\n</code></pre> <p></p> <p>Most of these QA bands are inputs to and outputs from the Atmospheric Correction (ATCOR), the process which converts radiance to atmospherically corrected reflectance. We will elaborate on these QA bands further, and encourage you to read more details about these data in the NEON Imaging Spectrometer Radiance to Reflectance Algorithm Theoretical Basis Document. For the purposes of this exercise, we will focus on the Weather Quality Indicator band.</p> <p>Exploring Other QA Bands</p> <p>Note that you can explore each of the QA bands, following similar steps below, adjusting the band names and values accordingly.</p>"},{"location":"tutorials/02_neon_reflectance_weather_qa/#read-in-the-weather-quality-indicator-band","title":"Read in the Weather Quality Indicator Band","text":"<p>The weather information, called <code>Weather_Quality_Indicator</code> is one of the most important pieces of QA information that is collected about the NIS data, as it has a direct impact on the reflectance values.</p> <p>These next lines of code pull out the <code>Weather_Quality_Indicator</code> band, select the \"green\" weather data from that band, and apply a mask to keep only the clear-weather data, which is saved to the variable <code>soapSDR_clear</code>.</p> <pre><code>// Extract a single band Weather Quality QA layer\nvar soapWeather = soapSDR.select(['Weather_Quality_Indicator']);\n\n// Select only the clear weather data (&lt;10% cloud cover)\nvar soapClearWeather = soapWeather.eq(1); // 1 = 0-10% cloud cover\n\n// Mask out all cloudy pixels from the SDR image\nvar soapSDR_clear = soapSDR.updateMask(soapClearWeather);\n</code></pre>"},{"location":"tutorials/02_neon_reflectance_weather_qa/#plot-the-weather-quality-band-data","title":"Plot the Weather Quality Band Data","text":"<p>For reference, we can plot the weather band data, using AOP's stop-light (red/yellow/green) color scheme, with the code below:</p> <pre><code>// center the map at the lat / lon of the site, set zoom to 12\nMap.setCenter(-119.25, 37.06, 11);\n\n// Define a palette for the weather - to match NEON AOP's weather color conventions\nvar gyrPalette = [\n  '00ff00', // green (&lt;10% cloud cover)\n  'ffff00', // yellow (10-50% cloud cover)\n  'ff0000' // red (&gt;50% cloud cover)\n];\n\n// Display the weather band (cloud conditions) with the green-yellow-red palette\nMap.addLayer(soapWeather,\n             {min: 1, max: 3, palette: gyrPalette, opacity: 0.3},\n             'SOAP 2019 Cloud Cover Map');\n</code></pre>"},{"location":"tutorials/02_neon_reflectance_weather_qa/#plot-the-clear-weather-reflectance-data","title":"Plot the Clear-Weather Reflectance Data","text":"<p>Finally, we can plot a true-color image of only the clear-weather data, from <code>soapSDR_clear</code> that we created earlier:</p> <pre><code>// Create a 3-band cloud-free image\nvar soapSDR_RGB = soapSDR_clear.select(['B053', 'B035', 'B019']);\n\n// Display the SDR image\nMap.addLayer(soapSDR_RGB, {min:103, max:1160}, 'SOAP 2019 Reflectance RGB');\n</code></pre> <p></p>"},{"location":"tutorials/02_neon_reflectance_weather_qa/#plot-acquisition-dates","title":"Plot Acquisition Dates","text":"<p>We can apply the same concepts to explore another one of the QA bands, this time let's look at the <code>Acquisition_Date</code>. This may be useful if you are trying to find the dates that correspond to field data you've collected, or you want to scale up to satellite data, for example. To determine the minimum and maximum dates, you can use <code>reduceRegion</code> with the reducer <code>ee.Reducer.minMax()</code> as follows. Then use these start and end date values in the visualization parameters.</p> <p>Managing Layer Display</p> <p>You may not wish to show every layer by default if you are plotting many layers. You can choose not to display a layer by default by including a \"0\" as the last input of <code>Map.addLayer</code>. Once you run the code, to toggle the layer on, find the Layers tab in the upper right corner of the Map Window and check the box to the left of the layer you want to display. You can click on the lock icon to make it so that the Layers full display stays open (by default it minimizes).</p> <pre><code>// Extract acquisition dates QA band\nvar soapDates = soapSDR.select(['Acquisition_Date']);\n\n// Get the minimum and maximum values of the soapDates band\nvar minMaxValues = soapDates.reduceRegion({reducer: ee.Reducer.minMax(),maxPixels: 1e10})\nprint('min and max dates', minMaxValues);\n\n// Map acquisition dates, don't display layer by default\nMap.addLayer(soapDates,\n            {min:20190612, max:20190616, opacity: 0.5},\n            'SOAP 2019 Acquisition Dates',0);\n</code></pre> <p></p>"},{"location":"tutorials/02_neon_reflectance_weather_qa/#other-qa-considerations","title":"Other QA Considerations","text":"<p>This lesson is intended to give you a quick introduction to some of the QA factors to consider when working with NEON AOP data, and is not meant to be comprehensive. When working with hyperspectral data, there are also invalid and noisy bands that you will likely want to remove before working with the full spectra (these bad bands are not unique to NEON hyperspectral data; for example NASA AVIRIS and EMIT hyperspectral data have the same limitations).</p> <p>Additional Resources</p> <p>Please see the lesson Plot spectral signatures of AOP Reflectance data in GEE for more details on these \"bad bands\". The Algorithm Theoretical Basis Documents (ATBDs) for the directional and bidirectional reflectance datasets, linked from the NEON Data Product Pages (DP3.30006.001, DP3.30006.002) and from the Quick Start Guides linked in the descriptions in the NEON Publisher Datasets pages on GEE, provide more comprehensive details on how these data products were derived and additional QA factors to consider.</p> <p>When pairing NEON data with satellite data, you will also need to factor in differences in how the datasets were processed, such as the atmospheric correction algorithm used, and other corrections that may or may not be applied (e.g. BRDF and topographic corrections) to both/all datasets you are integrating. If you can't find the information you need in the documentation, please reach out to NEON scientists using the NEON Contact Us form with any questions.</p>"},{"location":"tutorials/02_neon_reflectance_weather_qa/#summary","title":"Summary","text":"<p>In this lesson you learned how to read in Weather Quality Information from the Reflectance QA bands in GEE. You learned to mask data to keep only the imagery collected in the clearest sky conditions (&lt;10% cloud cover), and plot the three weather quality classes. You also learned how to find the other QA bands. Following a similar approach, you can explore each of the QA bands similarly. Filtering by the weather quality is an important first pre-processing step to working with NEON hyperspectral data, and is essential for interpreting the data and carrying out subsequent data analysis.</p>"},{"location":"tutorials/02_neon_reflectance_weather_qa/#get-lesson-code","title":"Get Lesson Code","text":"<p>AOP GEE SDR Weather Quality</p>"},{"location":"tutorials/03_plot_spectral_signature/","title":"Plot Spectral Signatures of AOP Reflectance Data in GEE","text":"\ud83d\udccb Tutorial Details\ud83c\udff7\ufe0f Topics &amp; Data <ul> <li>Duration: 30 minutes</li> <li>Level: Intermediate</li> <li>Authors: Bridget Hass, John Musinsky</li> <li>Contributors: Tristan Goulden, Lukas Straube</li> </ul> <ul> <li>Topics: lidar, hyperspectral, camera, remote-sensing</li> <li>Data Products: DP3.30006.001, DP3.30006.002, DP3.30010.001, DP3.30015.001, DP3.30024.001</li> </ul>"},{"location":"tutorials/03_plot_spectral_signature/#objectives","title":"Objectives","text":"<p>After completing this activity, you will be able to:</p> <ul> <li>Read in and map a single AOP Hyperspectral reflectance image at a NEON site</li> <li>Link spectral band numbers to wavelength values</li> <li>Create an interactive plot to display the spectral signature of a given pixel upon clicking</li> </ul>"},{"location":"tutorials/03_plot_spectral_signature/#requirements","title":"Requirements","text":"<ul> <li>Complete the following introductory AOP GEE tutorials:<ul> <li>Introduction to AOP Public Datasets in Google Earth Engine (GEE)</li> </ul> </li> <li>An understanding of hyperspectral data and AOP spectral data products. If this is your first time working with AOP hyperspectral data, we encourage you to start with the Intro to Working with Hyperspectral Remote Sensing Data tutorial. You do not need to follow along with the R code in those lessons, but at least read through to gain a better understanding NEON's spectral data products.</li> </ul>"},{"location":"tutorials/03_plot_spectral_signature/#read-in-the-aop-directional-reflectance-image","title":"Read in the AOP Directional Reflectance Image","text":"<p>As should be familiar by now from the previous tutorials in this series, we'll start by pulling in the AOP data. For this exercise we will only read directional reflectance data from SOAP collected in 2021:</p> <pre><code>// Filter image collection by date and site\nvar soapSDR = ee.ImageCollection(\"projects/neon-prod-earthengine/assets/HSI_REFL/001\")\n  .filterDate('2021-01-01', '2021-12-31')\n  .filterMetadata('NEON_SITE', 'equals', 'SOAP')\n  .first();\n\n// Create a 3-band true-color image\nvar soapSDR_RGB = soapSDR.select(['B053', 'B035', 'B019']);\n\n// Display the SDR image\nMap.addLayer(soapSDR_RGB, {min:103, max:1160}, 'SOAP 2021 Reflectance RGB');\n\n// Center the map around the soapSDR_RGB object, set zoom to 12\nMap.centerObject(soapSDR_RGB, 12);\n</code></pre>"},{"location":"tutorials/03_plot_spectral_signature/#extract-data-bands","title":"Extract Data Bands","text":"<p>Next we will extract only the \"data\" bands in order to plot the spectral information. The reflectance data contains 426 data bands, and a number of QA/Metadata bands that provide additional information that can be useful in interpreting and analyzing the data (such as the Weather Quality Information). For plotting the spectra, we only need the data bands.</p> <pre><code>// Pull out only the data bands (these all start with B, eg. B001)\nvar soapSDR_data = soapSDR.select('B.*')\nprint('SOAP SDR Data',soapSDR_data)\n\n// Read in the properties as a dictionary\nvar properties = soapSDR.toDictionary()\n</code></pre>"},{"location":"tutorials/03_plot_spectral_signature/#extract-wavelength-information-from-the-properties","title":"Extract Wavelength Information from the Properties","text":"<p>Similar to the code above, we can use a regular expression to pull out the wavelength information from the properties. The wavelength and Full Width Half Max (FWHM) information is stored in the properties starting with WL_FWHM_B. These are stored as strings, so the next step is to write a function that converts the string to a float, and only pulls out the center wavelength value (by splitting on the \",\" and pulling out only the first value). This is all we need for now, but if you needed the FWHM information, you could write a similar function. Lastly, we'll apply the function using GEE <code>.map</code> to pull out the wavelength information. We can then print some information about what we've extracted.</p> <pre><code>// Select the WL_FWHM_B*** band properties (using regex)\nvar wl_fwhm_dict = properties.select(['WL_FWHM_B+\\\\d{3}']);\n\n// Pull out the wavelength, fwhm values to a list\nvar wl_fwhm_list = wl_fwhm_dict.values()\nprint('Wavelength FWHM list:',wl_fwhm_list)\n\n// Function to pull out the wavelength values only and convert the string to float\nvar get_wavelengths = function(x) {\n  var str_split = ee.String(x).split(',')\n  var first_elem = ee.Number.parse((str_split.get(0)))\n  return first_elem\n}\n\n// apply the function to the wavelength full-width-half-max list\nvar wavelengths = wl_fwhm_list.map(get_wavelengths)\n\nprint('Wavelengths:',wavelengths)\nprint('# of data bands:',wavelengths.length())\n</code></pre>"},{"location":"tutorials/03_plot_spectral_signature/#interactively-plot-the-spectral-signature-of-a-pixel","title":"Interactively Plot the Spectral Signature of a Pixel","text":"<p>Lastly, we'll create a plot in the Map panel, and use the <code>Map.onClick</code> function to create a spectral signature of a given pixel that you click on. Most of the code below specifies formatting, figure labels, etc.</p> <pre><code>// Create a panel to hold the spectral signature plot\nvar panel = ui.Panel();\npanel.style().set({width: '600px',height: '300px',position: 'top-left'});\nMap.add(panel);\nMap.style().set('cursor', 'crosshair');\n\n// Create a function to draw a chart when a user clicks on the map.\nMap.onClick(function(coords) {\n  panel.clear();\n  var point = ee.Geometry.Point(coords.lon, coords.lat);\n    wavelengths.evaluate(function(wvlnghts) {\n      var chart = ui.Chart.image.regions({\n        image: soapSDR_data,\n        regions: point,\n        scale: 1,\n        seriesProperty: '\u03bb (nm)',\n        xLabels: wavelengths.getInfo()\n    });\n    chart.setOptions({\n      title: 'Reflectance',\n      hAxis: {title: 'Wavelength (nm)',\n      vAxis: {title: 'Reflectance'},\n      gridlines: { count: 5 }}\n    });\n    // Create and update the location label\n    var location = 'Longitude: ' + coords.lon.toFixed(2) + ' ' +\n                   'Latitude: ' + coords.lat.toFixed(2);\n    panel.widgets().set(1, ui.Label(location));\n    panel.add(chart);\n  })\n});\n</code></pre> <p>When you run this code (linked at the bottom), you will see the SOAP 2021 directional reflectance layer show up in the Map panel, along with an empty white figure panel in the lower left corner. When you click anywhere in the reflectance image, the empty figure panel will be populated with the spectral signature of the pixel you clicked on.</p> <p></p>"},{"location":"tutorials/03_plot_spectral_signature/#qa-considerations-bad-and-noisy-bands","title":"QA Considerations - Bad and Noisy Bands","text":"<p>Let's zoom in on the spectral signature figure to take a closer look. Specifically, you can easily spot some QA considerations that are important to factor in if you intend to work with all 426 bands of data.</p> <p></p>"},{"location":"tutorials/03_plot_spectral_signature/#water-vapor-band-windows","title":"Water Vapor Band Windows","text":"<p>We can see from the spectral profile above that the reflectance values dip to below zero around ~1400 nm and ~1800 nm. These are water vapor band windows, resulting from water vapor which absorbs light between wavelengths 1340-1445 nm and 1790-1955 nm. The atmospheric correction that converts radiance to reflectance subsequently results in a spike at these two bands, and are invalid values. They are set to -100 for the reflectance data in GEE.</p> <p>Water Vapor Bands in Different Datasets</p> <p>For more details on these bands, please refer to the Plot a Spectral Signature from Reflectance Data in Python tutorial. If you are working with hyperspectral data downloaded from the NEON Data Portal, these water vapor band windows are not set to -100, so this is one difference between the reflectance datasets on GEE and the datasets and the original hdf5 reflectance datasets.</p>"},{"location":"tutorials/03_plot_spectral_signature/#noisy-bands","title":"Noisy Bands","text":"<p>You may also notice that the reflectance values at the beginning (~380 nm) and end (~2500 nm) of the wavelength range spike up, relative to the other nearby bands. These are not a feature of the actual data. The first and last bands are more prone to have noisy values due to imperfect calibration of the sensor at the lowest and highest reflectance bands.</p> <p>Best Practice for Band Selection</p> <p>Best practice is to leave out the first and last 5-10 bands of data; you can inspect a spectral signature plot to determine how many bands to remove.</p>"},{"location":"tutorials/03_plot_spectral_signature/#summary","title":"Summary","text":"<p>In this lesson you learned how to read in wavelength information from the Surface Directional Reflectance properties in GEE, created functions to convert from one data format to another, and created an interactive plot to visualize the spectral signature of a selected pixel. You can quickly see how GEE is a powerful tool for interactive data visualization and exploratory analysis.</p>"},{"location":"tutorials/03_plot_spectral_signature/#get-lesson-code","title":"Get Lesson Code","text":"<p>AOP GEE Reflectance Plot Spectra</p>"},{"location":"tutorials/04_spectral_unmixing/","title":"Spectral Unmixing with NEON Hyperspectral Data","text":"\ud83d\udccb Tutorial Details\ud83c\udff7\ufe0f Topics &amp; Data <ul> <li>Duration: 10 minutes</li> <li>Level: Intermediate</li> <li>Authors: Samapriya Roy</li> </ul> <ul> <li>Topics: lidar, hyperspectral, camera, remote-sensing</li> <li>Data Products: DP3.30006.001, DP3.30006.002, DP3.30010.001, DP3.30015.001, DP3.30024.001</li> </ul>"},{"location":"tutorials/04_spectral_unmixing/#objectives","title":"Objectives","text":"<p>After completing this activity, you will be able to:</p> <ul> <li>Load and filter NEON hyperspectral imagery to remove problematic spectral bands</li> <li>Extract pure endmember spectra from representative land cover areas using reducers</li> <li>Apply constrained spectral unmixing with non-negativity and sum-to-one constraints</li> <li>Visualize fractional abundance maps for different land cover types</li> </ul>"},{"location":"tutorials/04_spectral_unmixing/#requirements","title":"Requirements","text":"<ul> <li>Basic understanding of Google Earth Engine and JavaScript</li> <li>Familiarity with hyperspectral remote sensing concepts</li> <li>Knowledge of spectral signatures and endmember analysis</li> <li>Understanding of NEON AOP data products</li> </ul>"},{"location":"tutorials/04_spectral_unmixing/#load-neon-hyperspectral-data","title":"Load NEON Hyperspectral Data","text":"<p>We'll start by loading the NEON Bidirectional Reflectance Hyperspectral Image Collection, which contains hundreds of spectral bands covering visible to near-infrared wavelengths:</p> <pre><code>// Load NEON Bidirectional Reflectance Hyperspectral Image Collection\nvar neon_bidirectional = ee.ImageCollection(\"projects/neon-prod-earthengine/assets/HSI_REFL/002\");\n\n// Explore available NEON sites in the collection\nprint('Available NEON sites:', neon_bidirectional.aggregate_histogram('NEON_SITE'));\n</code></pre>"},{"location":"tutorials/04_spectral_unmixing/#remove-problematic-spectral-bands","title":"Remove Problematic Spectral Bands","text":"<p>Hyperspectral data often contains bands affected by atmospheric absorption or sensor noise. We need to identify and remove these problematic bands before analysis:</p> <pre><code>// Define problematic spectral bands to exclude from analysis\n// These bands are typically excluded due to:\n// - Atmospheric absorption features (water vapor, oxygen)\n// - Sensor noise or calibration issues\n// - Edge effects at sensor boundaries\nvar bands_to_remove = [\n  'B195','B196','B197','B198','B199','B200','B201','B202','B203','B204','B205', // Water vapor ~940nm\n  'B287','B288','B289','B290','B291','B292','B293','B294','B295','B296','B297','B298', // Water vapor ~1130-1160nm\n  'B299','B300','B301','B302','B303','B304','B305','B306','B307','B308','B309','B310', // Continued water vapor\n  'B416','B417','B418','B419','B420','B421','B422','B423','B424','B425' // Water vapor ~1400nm\n];\n\nvar BAD_BAND_NAMES = ee.List(bands_to_remove);\n</code></pre> <p>Why Remove These Bands?</p> <p>Water vapor absorption windows around 940nm, 1130-1160nm, and 1400nm cause atmospheric interference that makes reflectance values unreliable. Removing these bands improves unmixing accuracy.</p>"},{"location":"tutorials/04_spectral_unmixing/#create-band-filtering-functions","title":"Create Band Filtering Functions","text":"<p>Next, we'll create utility functions to systematically remove the problematic bands from our imagery:</p> <pre><code>// Filter an image to retain only valid B-bands (spectral bands)\n// Excludes problematic bands and non-spectral bands (metadata, etc.)\nfunction selectValidBands(image) {\n  var allBands = image.bandNames(); // Get all band names\n  var bBands = allBands.filter(ee.Filter.stringStartsWith('item', 'B')); // Keep only B-bands\n  var validBands = bBands.filter(ee.Filter.inList('item', BAD_BAND_NAMES).not()); // Exclude bad bands\n  return image.select(validBands);\n}\n</code></pre>"},{"location":"tutorials/04_spectral_unmixing/#select-and-prepare-site-data","title":"Select and Prepare Site Data","text":"<p>We'll focus on SERC (Smithsonian Environmental Research Center), which represents a temperate deciduous forest ecosystem:</p> <pre><code>// Select and prepare data from SERC\nvar serc_site = neon_bidirectional.filter(ee.Filter.eq('NEON_SITE','SERC')).first();\nvar serc_filtered = selectValidBands(serc_site);\n\n// Display band filtering results for quality control\nprint('Original bands:', serc_site.bandNames().size());\nprint('Filtered bands:', serc_filtered.bandNames().size());\nprint('Number of excluded bands:', BAD_BAND_NAMES.size());\n\nvar image = serc_filtered;\nMap.centerObject(image);\n</code></pre> <p>Quality Control Check</p> <p>Always verify the number of bands before and after filtering to ensure the process worked correctly. NEON hyperspectral data typically has 426 bands, and you should retain most of them after removing problematic bands.</p>"},{"location":"tutorials/04_spectral_unmixing/#extract-pure-endmember-spectra-using-reducers","title":"Extract Pure Endmember Spectra Using Reducers","text":"<p>Endmembers represent the \"pure\" spectral signatures of different land cover types. We use the <code>reduceRegion</code> function to extract mean spectral signatures from representative areas:</p> <pre><code>// Define training data collections (these should be drawn as geometries)\nvar ag = /* color: #ffc82d */ ee.FeatureCollection([]);      // Agricultural areas\nvar water = /* color: #2564ff */ ee.FeatureCollection([]);   // Water bodies\nvar forest = /* color: #2ac219 */ ee.FeatureCollection([]);  // Forest cover\n\n// Extract water endmember - represents open water spectral signature\n// Typically shows low reflectance across all bands\nvar water_endmember = image.reduceRegion({\n  reducer: ee.Reducer.mean(),\n  geometry: water,\n  scale: 1, // Native NEON resolution (1m)\n  maxPixels: 1e9,\n  bestEffort: true\n}).values();\n\n// Extract forest canopy endmember - represents dense vegetation\n// Shows characteristic vegetation features: low red, high NIR reflectance\nvar canopy_endmember = image.reduceRegion({\n  reducer: ee.Reducer.mean(),\n  geometry: forest,\n  scale: 1,\n  maxPixels: 1e9,\n  bestEffort: true\n}).values();\n\n// Extract agricultural endmember - represents crop/agricultural areas\nvar ag_endmember = image.reduceRegion({\n  reducer: ee.Reducer.mean(),\n  geometry: ag,\n  scale: 1,\n  maxPixels: 1e9,\n  bestEffort: true\n}).values();\n\nprint('Water endmember values:', water_endmember);\n</code></pre> <p>Endmember Selection Strategy</p> <p>Choose the purest possible pixels for each land cover type. Water areas should be deep and clear, forest areas should be dense canopy, and agricultural areas should be uniform crop fields.</p>"},{"location":"tutorials/04_spectral_unmixing/#apply-constrained-spectral-unmixing","title":"Apply Constrained Spectral Unmixing","text":"<p>Now we'll perform the actual spectral unmixing using the extracted endmembers. The constraints ensure physically meaningful results:</p> <pre><code>// Organize endmembers for spectral unmixing\n// Order matters: results will correspond to this sequence\nvar endmembers = [ag_endmember, canopy_endmember, water_endmember];\nvar endmember_names = ['agriculture', 'forestry', 'water'];\n\n// Perform constrained spectral unmixing\n// Parameters:\n// - endmembers: Array of pure spectral signatures\n// - sumToOne: true = force fractions to sum to 1.0 (Sum-to-One Constraint)\n// - nonNegative: true = constrain fractions to be non-negative (Non-negativity Constraint)\nvar unmixed = image.unmix(endmembers, true, true).rename(endmember_names);\n\n// Display unmixed results on map\nMap.addLayer(unmixed, {}, 'Unmixed Fractional Abundances');\n</code></pre>"},{"location":"tutorials/04_spectral_unmixing/#understanding-the-constraints","title":"Understanding the Constraints","text":"<p>The constrained spectral unmixing applies two important physical constraints. You can read the paper here</p>"},{"location":"tutorials/04_spectral_unmixing/#non-negativity-constraint-anc","title":"Non-negativity Constraint (ANC)","text":"<p><pre><code>a_i \u2265 0 for all i\n</code></pre> Ensures that abundance fractions are physically meaningful (no negative abundances). You cannot have \"negative vegetation\" in a pixel.</p>"},{"location":"tutorials/04_spectral_unmixing/#sum-to-one-constraint-asc","title":"Sum-to-One Constraint (ASC)","text":"<p><pre><code>\u03a3 a_i = 1\n</code></pre> Ensures that abundance fractions sum to 100%, representing complete pixel coverage. This assumes that your endmembers represent all materials present in the scene.</p> <p>Interpreting Results</p> <p>Each band in the unmixed image represents fractional abundance (0-1) of the corresponding land cover type. Values closer to 1 indicate higher abundance of that material in the pixel.</p>"},{"location":"tutorials/04_spectral_unmixing/#visualization-and-analysis","title":"Visualization and Analysis","text":"<p>The resulting unmixed image contains three bands representing the fractional abundance of each endmember:</p> <ul> <li>Agriculture band: Shows areas with high crop/agricultural content</li> <li>Forestry band: Shows areas with high forest canopy content</li> <li>Water band: Shows areas with high water content</li> </ul> <pre><code>// Create individual abundance maps for better visualization\nvar ag_abundance = unmixed.select('agriculture');\nvar forest_abundance = unmixed.select('forestry');\nvar water_abundance = unmixed.select('water');\n\n// Add individual layers with appropriate color schemes\nMap.addLayer(ag_abundance, {min: 0, max: 1, palette: ['white', 'yellow']}, 'Agriculture Abundance');\nMap.addLayer(forest_abundance, {min: 0, max: 1, palette: ['white', 'green']}, 'Forest Abundance');\nMap.addLayer(water_abundance, {min: 0, max: 1, palette: ['white', 'blue']}, 'Water Abundance');\n</code></pre>"},{"location":"tutorials/04_spectral_unmixing/#summary","title":"Summary","text":"<p>In this lesson you learned how to perform constrained spectral unmixing with NEON hyperspectral data. Key steps included filtering out problematic spectral bands affected by atmospheric absorption, using reducers to extract pure endmember spectra from representative land cover areas, and applying constrained unmixing with non-negativity and sum-to-one constraints. The resulting fractional abundance maps provide quantitative estimates of land cover composition at the pixel level.</p>"}]}